# ðŸ§  How to Use Processed Data for ML/DL Projects

> **Document Purpose:**  
> This guide explains how to use the pre-processed outputs generated by the RBP-DSP pipeline for **machine learning (ML)** and **deep learning (DL)** tasks.  
> It covers dataset structure, metadata usage, reorganization steps, and practical examples to start model development.

---

## ðŸš€ 1. Overview

The **RBP-DSP** pipeline performs advanced digital signal processing on raw speech data.  
It includes:
- Audio extraction and normalization  
- Noise/dehum/dereverb cleaning  
- Voice activity segmentation (VAD)  
- Feature extraction (MFCC, Mel, LPC, Prosody, Phonetics)  
- TextGrid annotation scaffolds  
- Verification and KPI reports  

Once the pipeline finishes, you have a **scientifically curated dataset** ideal for ML/DL projects in speech, linguistics, and phonetics.

---

## ðŸ§© 2. Understanding the Directory Structure

After a full pipeline run, your workspace will look like this:

processed_data/
â”œâ”€â”€ 1_acquisition_preprocessing/
â”‚ â””â”€â”€ _resampled.wav
â”œâ”€â”€ 2_signal_separation_chain/
â”‚ â”œâ”€â”€ dehum_corrected/
â”‚ â”œâ”€â”€ dereverbed/
â”‚ â”œâ”€â”€ denoised/
â”‚ â”œâ”€â”€ normalized/
â”‚ â””â”€â”€ vad_segments/.wav â† speech-active chunks
â”œâ”€â”€ 3_phonetic_linguistic_features/
â”‚ â”œâ”€â”€ acoustic/.npz â† MFCC, Mel, Î”Î”, etc.
â”‚ â”œâ”€â”€ prosodic/.csv â† fâ‚€, energy, rhythm, duration
â”‚ â”œâ”€â”€ phonetic/.csv â† jitter, shimmer, formant stats
â”‚ â””â”€â”€ linguistic/.npy â† LPC, PNCC, etc.
â”œâ”€â”€ 4_annotation_alignment/
â”‚ â””â”€â”€ textgrids/.TextGrid â† time-aligned scaffolds
â””â”€â”€ 5_validation_verification/
â”œâ”€â”€ reports/scientific_evaluation.csv
â”œâ”€â”€ figures/.png
â””â”€â”€ evaluation_summary.txt

metadata/
â”œâ”€â”€ sources.csv â† original file metadata
â”œâ”€â”€ segments.csv â† all speech segments (timestamps)
â””â”€â”€ processing_log.jsonl â† stepwise pipeline logs




---

## ðŸ§± 3. From Processed Data to ML Dataset

The DSP pipeline produces **rich but hierarchical outputs**.  
To simplify ML use, we generate a flattened dataset using:




python scripts/prepare_dataset.py


This script gathers all valid audio, features, and metadata and creates:



dataset_ready/
â”œâ”€â”€ audio/
â”‚ â”œâ”€â”€ segment_0001.wav
â”‚ â”œâ”€â”€ segment_0002.wav
â”‚ â””â”€â”€ ...
â”œâ”€â”€ features/
â”‚ â”œâ”€â”€ segment_0001.npz
â”‚ â”œâ”€â”€ segment_0002.npz
â”‚ â””â”€â”€ ...
â”œâ”€â”€ labels.csv
â”œâ”€â”€ dataset_summary.txt
â”œâ”€â”€ dataset_statistics.png
â”œâ”€â”€ feature_scaler_mean.npy
â””â”€â”€ feature_scaler_std.npy




Now, **dataset_ready/** is fully standardized and portable.  
You can directly load it in Python, PyTorch, TensorFlow, or Scikit-learn workflows.

---

## ðŸ§¬ 4. Typical Use Cases

| Application Type | Input Features | Target or Label | Example Goal |
|------------------:|----------------|-----------------|---------------|
| **Speaker Classification** | MFCC, LPC, Fâ‚€ | Speaker ID | Identify who is speaking |
| **Emotion Recognition** | Prosody + MFCC | Emotion Label | Detect emotional tone |
| **Speech Quality Assessment** | Acoustic + Phonetic | PESQ, STOI | Predict perceived audio quality |
| **Accent/Dialect Analysis** | LPC + Formants | Region/Accent | Linguistic classification |
| **Phonetic Feature Modeling** | Jitter, Shimmer, Fâ‚€ | Phoneme attributes | Compare healthy vs disordered speech |
| **Automatic Speech Recognition (ASR)** | Audio + TextGrid | Word/phoneme alignment | Train end-to-end ASR models |

---

## ðŸ§° 5. How to Load and Use the Dataset in Python

### Example: Load and Explore Features

```python
import numpy as np
import pandas as pd

# Load labels
labels = pd.read_csv("dataset_ready/labels.csv")
print(labels.head())

# Load one sample
sample_id = labels.iloc[0]["segment_id"]
data = np.load(f"dataset_ready/features/{sample_id}.npz")

print("Available feature arrays:", data.files)
mfcc = data["mfcc"]
print("MFCC shape:", mfcc.shape)
```


Example: Train a Simple ML Model (Speech Quality Prediction)


```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np
import pandas as pd
from glob import glob

# Load mean MFCCs as feature vectors
files = sorted(glob("dataset_ready/features/*.npz"))
X = [np.load(f)["mfcc"].mean(axis=1) for f in files]
X = np.stack(X)

# Load target quality metric (e.g., PESQ)
df_eval = pd.read_csv("processed_data/5_validation_verification/reports/scientific_evaluation.csv")
y = df_eval["PESQ"].values[:len(X)]

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(f"RÂ² = {r2_score(y_test, y_pred):.3f}, RMSE = {mean_squared_error(y_test, y_pred, squared=False):.3f}")
```


Example: Use with PyTorch for Deep Learning
```python
import torch
from torch.utils.data import Dataset
import numpy as np
import soundfile as sf
import pandas as pd
from pathlib import Path

class SpeechDataset(Dataset):
    def __init__(self, root="dataset_ready"):
        self.df = pd.read_csv(Path(root) / "labels.csv")
        self.audio_dir = Path(root) / "audio"
        self.feat_dir = Path(root) / "features"

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        seg_id = row["segment_id"]
        feat = np.load(self.feat_dir / f"{seg_id}.npz")["mfcc"]
        feat_tensor = torch.tensor(feat, dtype=torch.float32)
        return feat_tensor, seg_id

dataset = SpeechDataset()
print(f"Loaded {len(dataset)} samples.")
```

This structure can directly feed models such as:

CNNs for spectral classification

LSTMs/Transformers for temporal features

Multimodal models combining acoustic and linguistic data


ðŸ“ˆ 6. Dataset Statistics and Integrity

Each dataset build also generates:

dataset_summary.txt â€” high-level stats (count, durations, total hours)

dataset_statistics.png â€” histogram of segment durations

feature_scaler_mean.npy, feature_scaler_std.npy â€” global normalization vectors

Use these for dataset validation before training:


```python
import numpy as np

mean = np.load("dataset_ready/feature_scaler_mean.npy")
std = np.load("dataset_ready/feature_scaler_std.npy")
print("Feature normalization mean/std shapes:", mean.shape, std.shape)
```


ðŸ”¬ 7. Academic / Research Use

This dataset is research-ready for:

Comparative DSP evaluations

Supervised or unsupervised ML experiments

Reproducibility studies in linguistics or signal science

Voice quality and speech pathology research

Phonetic, prosodic, or sociolinguistic analysis

When publishing, cite the RBP-DSP pipeline as the data preprocessing system
and reference its configuration (cfg/core_config.yml) for reproducibility.



ðŸ’¾ 8. Packaging for External Teams

Once the dataset is ready, you can compress and share it:

```
python scripts/prepare_dataset.py
tar -czvf dataset_ready_v1.tar.gz dataset_ready/
sha256sum dataset_ready_v1.tar.gz > dataset_ready_v1_sha256.txt
```


The receiving team can simply unpack it and begin model training.


ðŸ§­ 9. Summary

âœ… You now have a complete, clean, and scientifically validated speech dataset.
It can be used for:

Training deep neural networks

Benchmarking DSP methods

Linguistic or speech analysis

Student and PhD-level experiments

By following this workflow, your data pipeline remains:

Traceable (metadata and logs)

Reproducible (config-driven)

Modular (each stage can evolve independently)

Portable (dataset_ready/ is self-contained)

Authored by:
RBP-DSP Core Engineering Team
Â© 2025 | Research-Grade DSP + ML Workflow