# AI-Voice-Cloning---Dataset
Rasool Bux Palijo Digital Speech Processing Pipeline

> **Portable | Modular | Reproducible | Scientific**
>
> End-to-end system for *speech extraction, enhancement, phonetic analysis,*  
> and *AI-ready dataset generation* from public speeches, oral history, and field recordings.

---

## ðŸ“˜ Overview

**RBP-DSP** is a professional-grade digital speech processing pipeline built for  
scientific preservation and linguistic analysis of *Rasool Bux Palijoâ€™s* spoken legacy.  
It integrates modern **Digital Signal Processing (DSP)**, **Phonetics**, and **Machine Learning (ML)**  
principles into a unified, reproducible framework.

The system automatically performs:

1. ðŸŽ§ **Acquisition & Pre-processing**  
2. ðŸŽšï¸ **Signal Separation & Enhancement Chain**  
3. ðŸ”¬ **Phoneticâ€“Linguistic Feature Extraction**  
4. ðŸ—£ï¸ **Annotation & Alignment (TextGrid scaffolds)**  
5. ðŸ“Š **Validation & Verification**

All stages produce reproducible metadata, structured logs, and  
scientifically meaningful features for research, training, and archiving.

---

## ðŸ§© System Architecture

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 INPUT DOMAIN (raw_data)                â”‚
        â”‚  Public speeches, TV talks, field recordings           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚           1. ACQUISITION & PRE-PROCESSING              â”‚
        â”‚  - Resampling / Mono downmix / Amplitude calibration   â”‚
        â”‚  - Metadata registration                               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚            2. SIGNAL SEPARATION CHAIN                  â”‚
        â”‚  (Speech Isolation)                                   â”‚
        â”‚  - VAD + Diarization (WebRTC, pyannote)               â”‚
        â”‚  - Source Separation (Conv-TasNet / Demucs)           â”‚
        â”‚  - Dereverberation (WPE)                              â”‚
        â”‚  - Denoising (MMSE-LSA, RNNoise)                      â”‚
        â”‚  - De-hum / De-click / Dynamic Control                â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     3. PHONETIC-LINGUISTIC FEATURE EXTRACTION          â”‚
        â”‚  - Acoustic: MFCC, PNCC, LPC, LogMel                   â”‚
        â”‚  - Prosodic: F0, Energy, Duration, ToBI-LR             â”‚
        â”‚  - Phonetic: Formants, VOT, Jitter, Shimmer, CPP       â”‚
        â”‚  - Linguistic: Syllable timing, Accent, Rhythm         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        4. ANNOTATION & ALIGNMENT LAYERS                â”‚
        â”‚  - Word / Phone alignment (MFA / Kaldi)                â”‚
        â”‚  - Prosody & tone tiers (Praat / TextGrid)             â”‚
        â”‚  - Discourse / Dialect flags                           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚          5. VALIDATION & VERIFICATION MODULE           â”‚
        â”‚  - SNR / PESQ / STOI / SDR / LSD                       â”‚
        â”‚  - F0-contour correlation                              â”‚
        â”‚  - Spectrogram & envelope similarity                   â”‚
        â”‚  - Automated report generation                         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               OUTPUT: AI-READY DATASET                â”‚
        â”‚  Clean audio + aligned transcripts + full features     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


---

## ðŸ§® Core Mathematical & Algorithmic Foundations

### 1. Pre-processing and Resampling

Discrete-time resampling from \( f_{s0} \to f_{s1} \):

\[
y[n] = \sum_k x[k]\, h_r[n - k/R], \quad R = \frac{f_{s1}}{f_{s0}}
\]
where \( h_r[n] \) is a Kaiser-windowed sinc interpolation kernel.

### 2. Voice Activity Detection (VAD)

Energy-based frame decision:

\[
D(i) =
\begin{cases}
1, & 10\log_{10}\!\left(\frac{E_s(i)}{E_n(i)}\right) > \theta \\
0, & \text{otherwise}
\end{cases}
\]
implemented via WebRTC-VAD with adaptive thresholding.

### 3. Dereverberation (WPE)

Weighted Prediction Error (Yoshioka et al., 2012):

\[
x[n] = \sum_{m=0}^{M-1} g[m]\,s[n-m] + e[n]
\]
Minimize  
\(\displaystyle
E\!\left[\frac{|x[n] - \sum_{m=1}^M g[m]x[n-m]|^2}{\lambda[n]}\right]
\)
iteratively updating \( g[m] \) and \( \lambda[n] \).

### 4. MMSE-LSA Denoising (Ephraim & Malah 1984)

Posterior and prior SNR:

\[
\xi_k = \alpha\xi_{k-1} + (1-\alpha)\max(\gamma_k - 1, 0)
\]
Gain function:

\[
G(\xi,\gamma) = \frac{\xi}{1+\xi}\exp\!\left(\tfrac{1}{2}E_1(\nu)\right)
\]
applied in the STFT magnitude domain.

### 5. Multi-Notch De-hum Filtering

Each harmonic \( kf_h \):

\[
H_k(z)=\frac{1 - 2r\cos(2\pi f_h/f_s)z^{-1}+r^2z^{-2}}
{1 - 2\cos(2\pi f_h/f_s)z^{-1}+z^{-2}}
\]
with \( 0.95<r<0.99 \).

### 6. Loudness Normalization (EBU R128)

\[
g = 10^{(L_T - L_I)/20}, \quad y_n = g\,x_n,
\]
keeping \( \text{TruePeak} < -1\text{ dBFS} \).

### 7. Feature Extraction

- **MFCC / LogMel** via mel filterbanks & DCT  
- **LPC coefficients** from Burg predictor  
- **Prosody:** YIN for \(F_0\), short-term energy  
- **Phonetics:** Formants via LPC poles; Jitter, Shimmer, HNR via Praat  
- **Correlation metrics:** \(r = \text{corr}(F_{0,raw}, F_{0,proc})\)

---

## ðŸ§° Key Features & Highlights

| Module | Technique | Library |
|--------|------------|----------|
| Resampling | Kaiser-best & polyphase | `librosa`, `scipy` |
| De-hum | Multi-notch adaptive filter | custom |
| Dereverb | WPE (Pyroomacoustics) | `pyroomacoustics` |
| Denoise | MMSE-LSA | custom |
| Loudness | EBU R128 normalization | `pyloudnorm` |
| VAD | Frame-based WebRTC | `webrtcvad` |
| Acoustic features | MFCC, Mel, LPC | `librosa` |
| Phonetic features | Jitter, Shimmer, Formants | `parselmouth` |
| Evaluation | SNR, SI-SDR, STOI, PESQ | `pystoi`, `pesq` |

---

## ðŸ“‚ Directory Structure

RBP-DSP/
â”‚
â”œâ”€â”€ cfg/
â”‚ â”œâ”€â”€ core_config.yml
â”‚ â””â”€â”€ kpi_thresholds.yml
â”‚
â”œâ”€â”€ raw_data/ # Input speeches, videos
â”‚
â”œâ”€â”€ processed_data/
â”‚ â”œâ”€â”€ 1_acquisition_preprocessing/
â”‚ â”œâ”€â”€ 2_signal_separation_chain/
â”‚ â”œâ”€â”€ 3_phonetic_linguistic_features/
â”‚ â”œâ”€â”€ 4_annotation_alignment/
â”‚ â””â”€â”€ 5_validation_verification/
â”‚
â”œâ”€â”€ metadata/
â”‚ â”œâ”€â”€ sources.csv
â”‚ â”œâ”€â”€ segments.csv
â”‚ â”œâ”€â”€ features_index.csv
â”‚ â””â”€â”€ processing_log.jsonl
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ dsp_helpers.py
â”‚ â””â”€â”€ video_to_audio_converter.py
â”‚
â””â”€â”€ rbp_dsp_master_pipeline.py


---

## âš™ï¸ Configuration

Edit **`cfg/core_config.yml`**:

```yaml
project:
  samplerate: 16000

paths:
  raw: raw_data
  processed: processed_data
  metadata: metadata

dsp:
  dehum:
    enable: true
    mains_hz: 50
    harmonics: 5
    pole_r: 0.985
  dereverb:
    enable: true
    taps: 10
    delay: 3
    iterations: 3
  denoise:
    enable: true
    noise_floor_db: -65
  loudness:
    enable: true
    target_lufs: -23
    truepeak_dbfs: -1
  vad:
    enable: true
    mode: 2
    frame_ms: 20
    min_sec: 2.0
    max_sec: 20.0
    pad_sec: 0.05

---

## â–¶ï¸ Running the Pipeline

# Activate your virtual environment
source .venv/Scripts/activate      # Windows PowerShell

# Execute master pipeline
python rbp_dsp_master_pipeline.py


---

## Output Example

[INFO] Using configuration files:
 - cfg/core_config.yml
 - cfg/kpi_thresholds.yml

[1/5] Acquisition & Pre-processing
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.10s/it]

[2/5] Signal Separation Chain
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:58<00:00, 58.02s/it]

[3/5] Feature Extraction
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 532/532 [00:45<00:00, 11.8it/s]

[4/5] Annotation (TextGrid scaffolds)
[5/5] Validation & Verification
âœ… Pipeline completed successfully.

All results (audio, CSVs, figures) appear in processed_data/ and metadata/.

---
ðŸ§  Logging and Progress

Real-time progress via TQDM bars per stage.

Structured JSONL logs at:
metadata/processing_log.jsonl

Errors and warnings automatically appended to:
RBP-DSP/pipeline_run.log

Stage summaries saved as CSV.

âš—ï¸ Validation Metrics
Metric	Symbol	Description	Library
SNR	
10
log
â¡
10
âˆ¥
ð‘¦
âˆ¥
2
âˆ¥
ð‘¥
âˆ’
ð‘¦
âˆ¥
2
10log
10
	â€‹

âˆ¥xâˆ’yâˆ¥
2
âˆ¥yâˆ¥
2
	â€‹

	Signal clarity	internal
SI-SDR		Scale-invariant quality	internal
STOI		Intelligibility	pystoi
PESQ		Perceived quality (ITU-T P.862)	pesq
LSD		Log-Spectral Distortion	internal
Fâ‚€ Corr		Pitch-contour correlation	internal
ðŸ§‘â€ðŸ”¬ For Scientists

Provides reproducible DSP/phonetics workflow from raw audio to formant statistics.

Formant extraction via Burg LPC + Praat Formant (Burg).

Statistical robustness: Fâ‚€ mean/median/range; energy variance; formant averages.

CSV outputs are compatible with R, MATLAB, or Jupyter notebooks.

Suitable for phonetic correlation studies, dialectology, speech prosody research.

ðŸ‘©â€ðŸ’» For Engineers

Fully modular helper functions in scripts/dsp_helpers.py.

Easily extend pipeline with machine learning or speech recognition models.

Supports both headless CLI and IDE (PyCharm/VSCode) execution.

Automatic folder creation ensures portability.

Logging and error-tolerant design for production use.

ðŸŽ“ For Students

Learn real-world DSP by reading each function:

Resampling: anti-aliasing, polyphase filters

Dereverb: Weighted Prediction Error (WPE)

Noise reduction: MMSE-LSA algorithm

Feature extraction: MFCC, LPC, prosody

Evaluation: SNR, STOI, PESQ, LSD

Visualization: Spectrogram difference maps

A perfect foundation for courses in Speech Processing, Digital Signal Processing, or Machine Learning for Audio.

ðŸ§­ Scientific Impact

Bridges linguistics and signal processing for cultural preservation.

Enables cross-disciplinary datasets â€” combining speech, rhetoric, and phonetics.

Forms a basis for automatic speech scoring, style transfer, and expressive synthesis.

ðŸ§© Troubleshooting
Issue	Cause	Solution
No module named resampy	Missing dependency	pip install resampy
No compatible WPE in pyroomacoustics	Version mismatch	Upgrade: pip install -U pyroomacoustics
Praat 'To Pitch (ac)' failed	Parselmouth version conflict	pip install -U praat-parselmouth
Slow processing	Large files or low CPU	Use smaller sampling rate or disable heavy modules
ðŸ“š References

Ephraim, Y. & Malah, D. (1984). Speech enhancement using MMSE short-time spectral amplitude estimator. IEEE Trans. Acoustics, Speech, Signal Processing.

Yoshioka, T. et al. (2012). Dereverberation using Weighted Prediction Error. IEEE TASLP.

Boersma, P. & Weenink, D. (2024). Praat: Doing phonetics by computer.

EBU-R128 Loudness Recommendation.

ITU-T P.862 PESQ Standard.

ðŸ§­ Future Roadmap
Phase	Description
2.0	Integration with PyTorch Conv-TasNet / Demucs for true source separation
2.1	Kaldi or MFA-based forced alignment
2.2	MLflow & DVC integration for experiment tracking
3.0	Web dashboard + REST API for dataset search
3.1	Expansion to Sindhi & Urdu phonetic corpora
4.0	Neural vocoder resynthesis (HiFi-GAN)
ðŸ Citation

If you use this system in academic work, please cite:

Hussain, M. M. (2025).
RBP-DSP: A Modular Speech Processing Pipeline for Phonetic and Linguistic Analysis.
Internal Technical Report, Germany.

ðŸ§¾ License

MIT License Â© 2025 Muhammad Manzar Hussain
This software is open for scientific and educational purposes.

